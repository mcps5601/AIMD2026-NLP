{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 智慧運算技術導論 (Week 4 自然語言處理主題 - Day4)"
      ],
      "metadata": {
        "id": "3wY-TgA100Tf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 作業：RAG 系統\n",
        "- 使用資料集：[PubMedQA (pqa_labeled)](https://huggingface.co/datasets/qiaojin/PubMedQA/viewer/pqa_labeled)\n",
        "- 你的任務：完成所有的 `TODO`（一個 # Write your code here 代表有一行程式碼要寫）\n",
        "- 如果有任何問題歡迎舉手詢問老師\n",
        "- 雖然有些格子無需修改，但仍建議理解程式碼"
      ],
      "metadata": {
        "id": "o51Fa_qACZ3q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 分數判定\n",
        "- 完成所有 `TODO`: `60%`，任 1 `TODO` 為 `6分`。\n",
        "- 最後一格準確率達到 50% 以上: `10%`\n",
        "- 報告繳交: `30%`，內容需涵蓋：\n",
        "  - 請說明你所使用的 prompts\n",
        "  - 請說明本次作業的方法與你所使用的模型\n",
        "  - 請分享你遇到的問題或你覺得很困難的地方\n",
        "  - (如果有的話) 請說明你如何用 AI 完成此份作業\n",
        "- 繳交方式：\n",
        "  1. 準備一個資料夾\n",
        "  2. 放進去此檔案跟報告pdf\n",
        "  3. 將此資料夾壓縮為 `學號.zip`\n",
        "  4. 上傳至 Teams 作業區"
      ],
      "metadata": {
        "id": "dXvmB8ZzAkul"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 安裝與引入套件"
      ],
      "metadata": {
        "id": "FlDxAARF6UiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install 'aisuite[all]' gradio"
      ],
      "metadata": {
        "id": "RSHC4rFxC7RQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0b9ded97-b05d-473e-c130-9c0fd067efd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.50.0)\n",
            "Collecting aisuite[all]\n",
            "  Downloading aisuite-0.1.14-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting anthropic<0.31.0,>=0.30.1 (from aisuite[all])\n",
            "  Downloading anthropic-0.30.1-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting boto3<2.0.0,>=1.34.144 (from aisuite[all])\n",
            "  Downloading boto3-1.42.36-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting cerebras_cloud_sdk<2.0.0,>=1.19.0 (from aisuite[all])\n",
            "  Downloading cerebras_cloud_sdk-1.64.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting cohere<6.0.0,>=5.12.0 (from aisuite[all])\n",
            "  Downloading cohere-5.20.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting deepgram-sdk<6.0.0,>=5.0.0 (from aisuite[all])\n",
            "  Downloading deepgram_sdk-5.3.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting docstring-parser<0.16.0,>=0.15.0 (from aisuite[all])\n",
            "  Downloading docstring_parser-0.15-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: google-cloud-speech<3.0.0,>=2.33.0 in /usr/local/lib/python3.12/dist-packages (from aisuite[all]) (2.36.0)\n",
            "Collecting groq<0.10.0,>=0.9.0 (from aisuite[all])\n",
            "  Downloading groq-0.9.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting httpx<0.28.0,>=0.27.0 (from aisuite[all])\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting ibm-watsonx-ai<2.0.0,>=1.1.16 (from aisuite[all])\n",
            "  Downloading ibm_watsonx_ai-1.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: mcp<2.0.0,>=1.1.2 in /usr/local/lib/python3.12/dist-packages (from aisuite[all]) (1.25.0)\n",
            "Collecting mistralai<2.0.0,>=1.0.3 (from aisuite[all])\n",
            "  Downloading mistralai-1.10.1-py3-none-any.whl.metadata (33 kB)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from aisuite[all]) (1.6.0)\n",
            "Collecting numpy<2.0.0,>=1.24.0 (from aisuite[all])\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai<2.0.0,>=1.107.0 (from aisuite[all])\n",
            "  Downloading openai-1.109.1-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from aisuite[all]) (1.16.3)\n",
            "Collecting soundfile<0.13.0,>=0.12.1 (from aisuite[all])\n",
            "  Downloading soundfile-0.12.1-py2.py3-none-manylinux_2_31_x86_64.whl.metadata (14 kB)\n",
            "Collecting vertexai<2.0.0,>=1.63.0 (from aisuite[all])\n",
            "  Downloading vertexai-1.71.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.12.1)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.2.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.123.10)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (1.0.0)\n",
            "Requirement already satisfied: gradio-client==1.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.14.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.36.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<=2.12.3,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.12.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.21)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.3)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.14.13)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.50.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.21.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.40.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (1.3.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.0 in /usr/local/lib/python3.12/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.22.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
            "Collecting botocore<1.43.0,>=1.42.36 (from boto3<2.0.0,>=1.34.144->aisuite[all])\n",
            "  Downloading botocore-1.42.36-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0.0,>=1.34.144->aisuite[all])\n",
            "  Downloading jmespath-1.1.0-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.17.0,>=0.16.0 (from boto3<2.0.0,>=1.34.144->aisuite[all])\n",
            "  Downloading s3transfer-0.16.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting fastavro<2.0.0,>=1.9.4 (from cohere<6.0.0,>=5.12.0->aisuite[all])\n",
            "  Downloading fastavro-1.12.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: pydantic-core>=2.18.2 in /usr/local/lib/python3.12/dist-packages (from cohere<6.0.0,>=5.12.0->aisuite[all]) (2.41.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from cohere<6.0.0,>=5.12.0->aisuite[all]) (2.32.4)\n",
            "Collecting types-requests<3.0.0,>=2.0.0 (from cohere<6.0.0,>=5.12.0->aisuite[all])\n",
            "  Downloading types_requests-2.32.4.20260107-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi<1.0,>=0.115.2->gradio) (0.0.4)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-speech<3.0.0,>=2.33.0->aisuite[all]) (2.29.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-speech<3.0.0,>=2.33.0->aisuite[all]) (2.43.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-speech<3.0.0,>=2.33.0->aisuite[all]) (1.76.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-speech<3.0.0,>=2.33.0->aisuite[all]) (1.27.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-speech<3.0.0,>=2.33.0->aisuite[all]) (5.29.5)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<0.28.0,>=0.27.0->aisuite[all]) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<0.28.0,>=0.27.0->aisuite[all]) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->aisuite[all]) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.2.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.12/dist-packages (from ibm-watsonx-ai<2.0.0,>=1.1.16->aisuite[all]) (2.5.0)\n",
            "Collecting lomond (from ibm-watsonx-ai<2.0.0,>=1.1.16->aisuite[all])\n",
            "  Downloading lomond-0.3.3-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from ibm-watsonx-ai<2.0.0,>=1.1.16->aisuite[all]) (0.9.0)\n",
            "Collecting ibm-cos-sdk<2.15.0,>=2.12.0 (from ibm-watsonx-ai<2.0.0,>=1.1.16->aisuite[all])\n",
            "  Downloading ibm_cos_sdk-2.14.3.tar.gz (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.12/dist-packages (from ibm-watsonx-ai<2.0.0,>=1.1.16->aisuite[all]) (6.2.4)\n",
            "Requirement already satisfied: httpx-sse>=0.4 in /usr/local/lib/python3.12/dist-packages (from mcp<2.0.0,>=1.1.2->aisuite[all]) (0.4.3)\n",
            "Requirement already satisfied: jsonschema>=4.20.0 in /usr/local/lib/python3.12/dist-packages (from mcp<2.0.0,>=1.1.2->aisuite[all]) (4.26.0)\n",
            "Requirement already satisfied: pydantic-settings>=2.5.2 in /usr/local/lib/python3.12/dist-packages (from mcp<2.0.0,>=1.1.2->aisuite[all]) (2.12.0)\n",
            "Requirement already satisfied: pyjwt>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from pyjwt[crypto]>=2.10.1->mcp<2.0.0,>=1.1.2->aisuite[all]) (2.10.1)\n",
            "Requirement already satisfied: sse-starlette>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from mcp<2.0.0,>=1.1.2->aisuite[all]) (3.1.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from mcp<2.0.0,>=1.1.2->aisuite[all]) (0.4.2)\n",
            "Collecting eval-type-backport>=0.2.0 (from mistralai<2.0.0,>=1.0.3->aisuite[all])\n",
            "  Downloading eval_type_backport-0.3.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "INFO: pip is looking at multiple versions of mistralai to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting mistralai<2.0.0,>=1.0.3 (from aisuite[all])\n",
            "  Downloading mistralai-1.10.0-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.2/40.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mistralai-1.9.11-py3-none-any.whl.metadata (39 kB)\n",
            "  Downloading mistralai-1.9.10-py3-none-any.whl.metadata (37 kB)\n",
            "  Downloading mistralai-1.9.9-py3-none-any.whl.metadata (37 kB)\n",
            "  Downloading mistralai-1.9.8-py3-none-any.whl.metadata (37 kB)\n",
            "  Downloading mistralai-1.9.7-py3-none-any.whl.metadata (37 kB)\n",
            "  Downloading mistralai-1.9.6-py3-none-any.whl.metadata (37 kB)\n",
            "INFO: pip is still looking at multiple versions of mistralai to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading mistralai-1.9.3-py3-none-any.whl.metadata (37 kB)\n",
            "  Downloading mistralai-1.9.2-py3-none-any.whl.metadata (36 kB)\n",
            "  Downloading mistralai-1.9.1-py3-none-any.whl.metadata (33 kB)\n",
            "  Downloading mistralai-1.8.2-py3-none-any.whl.metadata (33 kB)\n",
            "  Downloading mistralai-1.8.1-py3-none-any.whl.metadata (33 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading mistralai-1.8.0-py3-none-any.whl.metadata (33 kB)\n",
            "  Downloading mistralai-1.7.1-py3-none-any.whl.metadata (30 kB)\n",
            "  Downloading mistralai-1.7.0-py3-none-any.whl.metadata (30 kB)\n",
            "  Downloading mistralai-1.6.0-py3-none-any.whl.metadata (30 kB)\n",
            "  Downloading mistralai-1.5.2-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting jsonpath-python>=1.0.6 (from mistralai<2.0.0,>=1.0.3->aisuite[all])\n",
            "  Downloading jsonpath_python-1.1.4-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from mistralai<2.0.0,>=1.0.3->aisuite[all]) (2.9.0.post0)\n",
            "Collecting typing-inspect>=0.9.0 (from mistralai<2.0.0,>=1.0.3->aisuite[all])\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile<0.13.0,>=0.12.1->aisuite[all]) (2.0.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Collecting google-cloud-aiplatform==1.71.1 (from google-cloud-aiplatform[all]==1.71.1->vertexai<2.0.0,>=1.63.0->aisuite[all])\n",
            "  Downloading google_cloud_aiplatform-1.71.1-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Collecting google-cloud-storage<3.0.0dev,>=1.32.0 (from google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai<2.0.0,>=1.63.0->aisuite[all])\n",
            "  Downloading google_cloud_storage-2.19.0-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai<2.0.0,>=1.63.0->aisuite[all]) (3.40.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai<2.0.0,>=1.63.0->aisuite[all]) (1.16.0)\n",
            "Requirement already satisfied: shapely<3.0.0dev in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai<2.0.0,>=1.63.0->aisuite[all]) (2.1.2)\n",
            "\u001b[33mWARNING: google-cloud-aiplatform 1.71.1 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile<0.13.0,>=0.12.1->aisuite[all]) (2.23)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-speech<3.0.0,>=2.33.0->aisuite[all]) (1.72.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-speech<3.0.0,>=2.33.0->aisuite[all]) (1.71.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-speech<3.0.0,>=2.33.0->aisuite[all]) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-speech<3.0.0,>=2.33.0->aisuite[all]) (4.9.1)\n",
            "Collecting ibm-cos-sdk-core==2.14.3 (from ibm-cos-sdk<2.15.0,>=2.12.0->ibm-watsonx-ai<2.0.0,>=1.1.16->aisuite[all])\n",
            "  Downloading ibm_cos_sdk_core-2.14.3.tar.gz (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ibm-cos-sdk-s3transfer==2.14.3 (from ibm-cos-sdk<2.15.0,>=2.12.0->ibm-watsonx-ai<2.0.0,>=1.1.16->aisuite[all])\n",
            "  Downloading ibm_cos_sdk_s3transfer-2.14.3.tar.gz (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.6/139.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0.0,>=1.34.144->aisuite[all])\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.1.2->aisuite[all]) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.1.2->aisuite[all]) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.1.2->aisuite[all]) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.1.2->aisuite[all]) (0.30.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings>=2.5.2->mcp<2.0.0,>=1.1.2->aisuite[all]) (1.2.1)\n",
            "Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.12/dist-packages (from pyjwt[crypto]>=2.10.1->mcp<2.0.0,>=1.1.2->aisuite[all]) (43.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->mistralai<2.0.0,>=1.0.3->aisuite[all]) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->cohere<6.0.0,>=5.12.0->aisuite[all]) (3.4.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.9.0->mistralai<2.0.0,>=1.0.3->aisuite[all])\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai<2.0.0,>=1.63.0->aisuite[all]) (2.5.0)\n",
            "Requirement already satisfied: google-resumable-media<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai<2.0.0,>=1.63.0->aisuite[all]) (2.8.0)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0,>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai<2.0.0,>=1.63.0->aisuite[all]) (0.14.3)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.71.1->google-cloud-aiplatform[all]==1.71.1->vertexai<2.0.0,>=1.63.0->aisuite[all]) (1.8.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-speech<3.0.0,>=2.33.0->aisuite[all]) (0.6.2)\n",
            "Downloading anthropic-0.30.1-py3-none-any.whl (863 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m863.9/863.9 kB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.42.36-py3-none-any.whl (140 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cerebras_cloud_sdk-1.64.1-py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.8/97.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cohere-5.20.2-py3-none-any.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.0/319.0 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading deepgram_sdk-5.3.1-py3-none-any.whl (505 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m505.9/505.9 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n",
            "Downloading groq-0.9.0-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ibm_watsonx_ai-1.5.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mistralai-1.5.2-py3-none-any.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.1/278.1 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.109.1-py3-none-any.whl (948 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m948.6/948.6 kB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading soundfile-0.12.1-py2.py3-none-manylinux_2_31_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading vertexai-1.71.1-py3-none-any.whl (7.3 kB)\n",
            "Downloading google_cloud_aiplatform-1.71.1-py2.py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aisuite-0.1.14-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.4/84.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.42.36-py3-none-any.whl (14.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading eval_type_backport-0.3.1-py3-none-any.whl (6.1 kB)\n",
            "Downloading fastavro-1.12.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading jsonpath_python-1.1.4-py3-none-any.whl (12 kB)\n",
            "Downloading s3transfer-0.16.0-py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_requests-2.32.4.20260107-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading lomond-0.3.3-py2.py3-none-any.whl (35 kB)\n",
            "Downloading google_cloud_storage-2.19.0-py2.py3-none-any.whl (131 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.8/131.8 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Building wheels for collected packages: ibm-cos-sdk, ibm-cos-sdk-core, ibm-cos-sdk-s3transfer\n",
            "  Building wheel for ibm-cos-sdk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ibm-cos-sdk: filename=ibm_cos_sdk-2.14.3-py3-none-any.whl size=77232 sha256=859965c172a507673283f42db071197d284edded49c8044ba47a8b2957ee914f\n",
            "  Stored in directory: /root/.cache/pip/wheels/cc/2f/6f/125918ad46d280d3bea58edf99f0757888ef6e7999db4b73b7\n",
            "  Building wheel for ibm-cos-sdk-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ibm-cos-sdk-core: filename=ibm_cos_sdk_core-2.14.3-py3-none-any.whl size=662101 sha256=0a2b7db0c33c6d3bd82365f38a9dfbb36574a9869d74b9f01a76f741c40fec95\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/53/13/7c8fdeebdb847995d8ef349b4f695c595d8d31b30ae2a07ea2\n",
            "  Building wheel for ibm-cos-sdk-s3transfer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ibm-cos-sdk-s3transfer: filename=ibm_cos_sdk_s3transfer-2.14.3-py3-none-any.whl size=90203 sha256=bcd48201f59e087913817c8c911b90af47fb36ab6512af699c38ca0ca63855a5\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/8b/10/0346c5a955b48b7fca50a8a42de309546b22899b7e8c1da8a5\n",
            "Successfully built ibm-cos-sdk ibm-cos-sdk-core ibm-cos-sdk-s3transfer\n",
            "Installing collected packages: types-requests, numpy, mypy-extensions, lomond, jsonpath-python, jmespath, fastavro, eval-type-backport, docstring-parser, typing-inspect, soundfile, ibm-cos-sdk-core, httpx, botocore, s3transfer, openai, mistralai, ibm-cos-sdk-s3transfer, groq, deepgram-sdk, cerebras_cloud_sdk, aisuite, ibm-cos-sdk, cohere, boto3, anthropic, ibm-watsonx-ai, google-cloud-storage, google-cloud-aiplatform, vertexai\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: docstring-parser\n",
            "    Found existing installation: docstring_parser 0.17.0\n",
            "    Uninstalling docstring_parser-0.17.0:\n",
            "      Successfully uninstalled docstring_parser-0.17.0\n",
            "  Attempting uninstall: soundfile\n",
            "    Found existing installation: soundfile 0.13.1\n",
            "    Uninstalling soundfile-0.13.1:\n",
            "      Successfully uninstalled soundfile-0.13.1\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 2.15.0\n",
            "    Uninstalling openai-2.15.0:\n",
            "      Successfully uninstalled openai-2.15.0\n",
            "  Attempting uninstall: google-cloud-storage\n",
            "    Found existing installation: google-cloud-storage 3.8.0\n",
            "    Uninstalling google-cloud-storage-3.8.0:\n",
            "      Successfully uninstalled google-cloud-storage-3.8.0\n",
            "  Attempting uninstall: google-cloud-aiplatform\n",
            "    Found existing installation: google-cloud-aiplatform 1.130.0\n",
            "    Uninstalling google-cloud-aiplatform-1.130.0:\n",
            "      Successfully uninstalled google-cloud-aiplatform-1.130.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "google-genai 1.55.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.27.2 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "firebase-admin 6.9.0 requires httpx[http2]==0.28.1, but you have httpx 0.27.2 which is incompatible.\n",
            "pytensor 2.36.3 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "tobler 0.13.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "rasterio 1.5.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "google-adk 1.21.0 requires google-cloud-aiplatform[agent-engines]<2.0.0,>=1.125.0, but you have google-cloud-aiplatform 1.71.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aisuite-0.1.14 anthropic-0.30.1 boto3-1.42.36 botocore-1.42.36 cerebras_cloud_sdk-1.64.1 cohere-5.20.2 deepgram-sdk-5.3.1 docstring-parser-0.15 eval-type-backport-0.3.1 fastavro-1.12.1 google-cloud-aiplatform-1.71.1 google-cloud-storage-2.19.0 groq-0.9.0 httpx-0.27.2 ibm-cos-sdk-2.14.3 ibm-cos-sdk-core-2.14.3 ibm-cos-sdk-s3transfer-2.14.3 ibm-watsonx-ai-1.5.0 jmespath-1.0.1 jsonpath-python-1.1.4 lomond-0.3.3 mistralai-1.5.2 mypy-extensions-1.1.0 numpy-1.26.4 openai-1.109.1 s3transfer-0.16.0 soundfile-0.12.1 types-requests-2.32.4.20260107 typing-inspect-0.9.0 vertexai-1.71.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "httpx",
                  "numpy"
                ]
              },
              "id": "65ff68a40c6240cea9d2e40eb071b477"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== 0. 引入套件 ====================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import gradio as gr\n",
        "import aisuite as ai\n",
        "from google.colab import userdata\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "6D-KRzAUBeVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== 0. 模型與資料設定 ====================\n",
        "\n",
        "MODEL_NAME = \"groq:openai/gpt-oss-120b\" # 可以換成你喜歡的模型\n",
        "NUM_TEST_SAMPLES = 10                   # 請勿修改此數字，我們只測 10 筆就好"
      ],
      "metadata": {
        "id": "BqX7VRFR0rF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== 1. 載入資料（此格無需修改）====================\n",
        "\n",
        "print(\"載入 PubMedQA 資料...\")\n",
        "with open(\"pubmedqa_100.json\", 'r', encoding=\"utf-8\") as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "with open(\"pubmedqa_train_corpus.json\", 'r', encoding=\"utf-8\") as f:\n",
        "    all_data = json.load(f)\n",
        "\n",
        "print(f\"✅ 載入完成！共 {len(all_data)} 筆語料庫（Corpus）\")\n",
        "print(f\"✅ 載入完成！共 {len(test_data)} 筆醫學問答\")\n",
        "print(\"\\n範例資料：\")\n",
        "print(f\"問題: {test_data[0]['question']}\")\n",
        "print(f\"答案: {test_data[0]['answer']}\")\n",
        "print()"
      ],
      "metadata": {
        "id": "ex-A1o2cB-9B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97caabd4-83b2-4193-e4df-563d19a82f5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "載入 PubMedQA 資料...\n",
            "✅ 載入完成！共 1000 筆語料庫（Corpus）\n",
            "✅ 載入完成！共 100 筆醫學問答\n",
            "\n",
            "範例資料：\n",
            "問題: Do mitochondria play a role in remodelling lace plant leaves during programmed cell death?\n",
            "答案: yes\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== 2. 設定 Groq API ====================\n",
        "\n",
        "# TODO1: 請註冊 Groq 帳號，並填入自己的 API_KEY\n",
        "os.environ['GROQ_API_KEY'] = \"YOUR_API_KEY\"\n",
        "# 或是使用 Colab Secret Key 方式\n",
        "# os.environ['GROQ_API_KEY'] = userdata.get('groq')\n",
        "\n",
        "client = ai.Client()\n",
        "print(\"✅ Groq 設定完成\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4VFZ6MVBcoJ",
        "outputId": "e39a1f73-3321-45be-ae6b-3e8fd9906d51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Groq 設定完成\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF"
      ],
      "metadata": {
        "id": "u_lCVHjSCLDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== 3. 建立 TF-IDF 檢索器 ====================\n",
        "\n",
        "corpus_train = [item['context'] for item in all_data]\n",
        "corpus_test = [item['context'] for item in test_data]\n",
        "\n",
        "# TODO2: 請創建 TF-IDF 檢索器變數\n",
        "vectorizer = # Write your code here\n",
        "\n",
        "# TODO3: 請建立 TF-IDF 檢索器\n",
        "# Write your code here\n",
        "tfidf_matrix = vectorizer.fit_transform(corpus_test)\n",
        "\n",
        "print(\"✅ TF-IDF 檢索器建立完成\")\n",
        "print(f\"   - 詞彙量: {len(vectorizer.get_feature_names_out())}\")\n",
        "print(f\"   - 文檔數: {tfidf_matrix.shape[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdo-g4asBlI_",
        "outputId": "3d0bbbc6-dc74-4e05-da30-80fe822a195f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ TF-IDF 檢索器建立完成\n",
            "   - 詞彙量: 3517\n",
            "   - 文檔數: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== 4. 檢索函數（此格無需修改）====================\n",
        "\n",
        "def retrieve_documents(retriever: TfidfVectorizer, query: str, top_k: int = 3):\n",
        "    \"\"\"使用 TF-IDF 檢索最相關的文檔\"\"\"\n",
        "\n",
        "    # TODO4：請利用 retriever 對 query 進行向量轉換\n",
        "    query_vec = # Write your code here\n",
        "\n",
        "    similarities = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
        "    top_indices = similarities.argsort()[-top_k:][::-1]\n",
        "\n",
        "    # 整理檢索結果\n",
        "    results = []\n",
        "    for idx in top_indices:\n",
        "        results.append({\n",
        "            \"rank\": len(results) + 1,\n",
        "            \"similarity\": float(similarities[idx]),\n",
        "            \"context\": test_data[idx]['context'],\n",
        "            \"question\": test_data[idx]['question']\n",
        "        })\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "CjCerZVf1_AS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RAG 系統"
      ],
      "metadata": {
        "id": "8UuILCRPCULY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== 5. LLM 生成函數 ====================\n",
        "\n",
        "def generate_answer(model: str, question: str, retrieved_docs: list) -> str:\n",
        "    \"\"\"使用 Groq LLM 根據檢索到的文檔生成答案\"\"\"\n",
        "\n",
        "    # TODO5: 請針對參考文件區塊，設計你的 prompt\n",
        "    context = \"\"\n",
        "    for doc in retrieved_docs:\n",
        "        context += f\"{doc['context']}\\n\\n\"  # Write your code here（對此行進行修改）\n",
        "\n",
        "    # TODO6: 請設計你的 prompt，讓模型知道參考文件區塊的提示\n",
        "    # TODO7: 請修改 Question: 那行以下的 prompt，讓模型知道它應該回答什麼\n",
        "    prompt = f\"\"\"Based on the following medical literature ... # Write your code here（對此行進行修改）\n",
        "\n",
        "Medical Literature:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "                                          # Write your code here（你不一定要換行）\n",
        "Format:                                   # Write your code here（對此行進行修改）\n",
        "1. First answer 'yes', 'no', or 'maybe'.  # Write your code here（對此行進行修改）\n",
        "2. Then                                   # Write your code here（對此行進行修改）\n",
        "\"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            # TODO8: 請給予模型 system prompt\n",
        "            # Write your code here\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        # TODO9: 請設定最長 tokens 數目\n",
        "        max_tokens=, # Write your code here\n",
        "        # TODO10: 請設定溫度係數\n",
        "        temperature=, # Write your code here\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "dME0D3Q12BuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== 6. 完整的 RAG 流程（此格無需修改） ====================\n",
        "\n",
        "def rag_system(retriever: TfidfVectorizer, model: str, question: str, top_k: int = 3) -> str:\n",
        "    \"\"\"完整的 RAG 系統：檢索 + 生成\"\"\"\n",
        "\n",
        "    print(f\"\\n🔍 正在檢索相關文獻...\")\n",
        "\n",
        "    # Step 1: 檢索\n",
        "    retrieved_docs = retrieve_documents(retriever, question, top_k=top_k)\n",
        "\n",
        "    print(f\"✅ 找到 {len(retrieved_docs)} 篇相關文獻\")\n",
        "    for doc in retrieved_docs:\n",
        "        print(f\"  - 相似度 {doc['similarity']:.3f}: {doc['context'][:80]}...\")\n",
        "\n",
        "    # Step 2: 生成\n",
        "    print(f\"\\n🤖 正在生成答案...\")\n",
        "    answer = generate_answer(model, question, retrieved_docs)\n",
        "\n",
        "    return answer"
      ],
      "metadata": {
        "id": "H1LBZn8I2G-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== 7. 測試單一問題（此格無需修改） ====================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"測試 RAG 系統\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "test_question = test_data[0]['question']\n",
        "print(f\"\\n問題: {test_question}\")\n",
        "\n",
        "answer = rag_system(\n",
        "    retriever=vectorizer,\n",
        "    model=MODEL_NAME,\n",
        "    question=test_question,\n",
        "    top_k=3\n",
        ")\n",
        "\n",
        "print(f\"\\n💡 RAG 系統的答案：\")\n",
        "print(answer)\n",
        "print(f\"\\n📚 標準答案：{test_data[0]['answer']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSdvRzwy2QL4",
        "outputId": "7a79b636-c420-41a6-b14c-c667d914f3c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "測試 RAG 系統\n",
            "============================================================\n",
            "\n",
            "問題: Do mitochondria play a role in remodelling lace plant leaves during programmed cell death?\n",
            "\n",
            "🔍 正在檢索相關文獻...\n",
            "✅ 找到 3 篇相關文獻\n",
            "  - 相似度 0.276: Programmed cell death (PCD) is the regulated death of cells within an organism. ...\n",
            "  - 相似度 0.081: The endogenous estradiol metabolite, 2-methoxyestradiol (2ME), has been shown to...\n",
            "  - 相似度 0.070: Using murine models, we have shown that the lysosomotropic amine, chloroquine, i...\n",
            "\n",
            "🤖 正在生成答案...\n",
            "\n",
            "💡 RAG 系統的答案：\n",
            "maybe  \n",
            "The excerpt from Document 1 describes the spatial pattern of PCD in lace‑plant leaf areoles but does not mention mitochondria or their involvement in the remodeling process. While mitochondria are known to participate in many plant programmed cell‑death pathways, the provided literature gives no direct evidence for or against their role in lace‑plant leaf perforation, leaving the answer uncertain.\n",
            "\n",
            "📚 標準答案：yes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== 8. 取得模型對於前10筆資料的預測結果（此格無需修改） ====================\n",
        "\n",
        "predictions = []\n",
        "\n",
        "for i in range(NUM_TEST_SAMPLES):\n",
        "    correct = 0\n",
        "    question = test_data[i]['question']\n",
        "    ground_truth = test_data[i]['answer'].lower()\n",
        "    context = test_data[i]['context']\n",
        "\n",
        "    answer = rag_system(\n",
        "        retriever=vectorizer,\n",
        "        model=MODEL_NAME,\n",
        "        question=test_question,\n",
        "        top_k=3\n",
        "    )\n",
        "    predictions.append(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTMAb4ah4Ayu",
        "outputId": "7457c994-eb07-4def-9140-3e5c0790fd00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔍 正在檢索相關文獻...\n",
            "✅ 找到 3 篇相關文獻\n",
            "  - 相似度 0.276: Programmed cell death (PCD) is the regulated death of cells within an organism. ...\n",
            "  - 相似度 0.081: The endogenous estradiol metabolite, 2-methoxyestradiol (2ME), has been shown to...\n",
            "  - 相似度 0.070: Using murine models, we have shown that the lysosomotropic amine, chloroquine, i...\n",
            "\n",
            "🤖 正在生成答案...\n",
            "\n",
            "🔍 正在檢索相關文獻...\n",
            "✅ 找到 3 篇相關文獻\n",
            "  - 相似度 0.276: Programmed cell death (PCD) is the regulated death of cells within an organism. ...\n",
            "  - 相似度 0.081: The endogenous estradiol metabolite, 2-methoxyestradiol (2ME), has been shown to...\n",
            "  - 相似度 0.070: Using murine models, we have shown that the lysosomotropic amine, chloroquine, i...\n",
            "\n",
            "🤖 正在生成答案...\n",
            "\n",
            "🔍 正在檢索相關文獻...\n",
            "✅ 找到 3 篇相關文獻\n",
            "  - 相似度 0.276: Programmed cell death (PCD) is the regulated death of cells within an organism. ...\n",
            "  - 相似度 0.081: The endogenous estradiol metabolite, 2-methoxyestradiol (2ME), has been shown to...\n",
            "  - 相似度 0.070: Using murine models, we have shown that the lysosomotropic amine, chloroquine, i...\n",
            "\n",
            "🤖 正在生成答案...\n",
            "\n",
            "🔍 正在檢索相關文獻...\n",
            "✅ 找到 3 篇相關文獻\n",
            "  - 相似度 0.276: Programmed cell death (PCD) is the regulated death of cells within an organism. ...\n",
            "  - 相似度 0.081: The endogenous estradiol metabolite, 2-methoxyestradiol (2ME), has been shown to...\n",
            "  - 相似度 0.070: Using murine models, we have shown that the lysosomotropic amine, chloroquine, i...\n",
            "\n",
            "🤖 正在生成答案...\n",
            "\n",
            "🔍 正在檢索相關文獻...\n",
            "✅ 找到 3 篇相關文獻\n",
            "  - 相似度 0.276: Programmed cell death (PCD) is the regulated death of cells within an organism. ...\n",
            "  - 相似度 0.081: The endogenous estradiol metabolite, 2-methoxyestradiol (2ME), has been shown to...\n",
            "  - 相似度 0.070: Using murine models, we have shown that the lysosomotropic amine, chloroquine, i...\n",
            "\n",
            "🤖 正在生成答案...\n",
            "\n",
            "🔍 正在檢索相關文獻...\n",
            "✅ 找到 3 篇相關文獻\n",
            "  - 相似度 0.276: Programmed cell death (PCD) is the regulated death of cells within an organism. ...\n",
            "  - 相似度 0.081: The endogenous estradiol metabolite, 2-methoxyestradiol (2ME), has been shown to...\n",
            "  - 相似度 0.070: Using murine models, we have shown that the lysosomotropic amine, chloroquine, i...\n",
            "\n",
            "🤖 正在生成答案...\n",
            "\n",
            "🔍 正在檢索相關文獻...\n",
            "✅ 找到 3 篇相關文獻\n",
            "  - 相似度 0.276: Programmed cell death (PCD) is the regulated death of cells within an organism. ...\n",
            "  - 相似度 0.081: The endogenous estradiol metabolite, 2-methoxyestradiol (2ME), has been shown to...\n",
            "  - 相似度 0.070: Using murine models, we have shown that the lysosomotropic amine, chloroquine, i...\n",
            "\n",
            "🤖 正在生成答案...\n",
            "\n",
            "🔍 正在檢索相關文獻...\n",
            "✅ 找到 3 篇相關文獻\n",
            "  - 相似度 0.276: Programmed cell death (PCD) is the regulated death of cells within an organism. ...\n",
            "  - 相似度 0.081: The endogenous estradiol metabolite, 2-methoxyestradiol (2ME), has been shown to...\n",
            "  - 相似度 0.070: Using murine models, we have shown that the lysosomotropic amine, chloroquine, i...\n",
            "\n",
            "🤖 正在生成答案...\n",
            "\n",
            "🔍 正在檢索相關文獻...\n",
            "✅ 找到 3 篇相關文獻\n",
            "  - 相似度 0.276: Programmed cell death (PCD) is the regulated death of cells within an organism. ...\n",
            "  - 相似度 0.081: The endogenous estradiol metabolite, 2-methoxyestradiol (2ME), has been shown to...\n",
            "  - 相似度 0.070: Using murine models, we have shown that the lysosomotropic amine, chloroquine, i...\n",
            "\n",
            "🤖 正在生成答案...\n",
            "\n",
            "🔍 正在檢索相關文獻...\n",
            "✅ 找到 3 篇相關文獻\n",
            "  - 相似度 0.276: Programmed cell death (PCD) is the regulated death of cells within an organism. ...\n",
            "  - 相似度 0.081: The endogenous estradiol metabolite, 2-methoxyestradiol (2ME), has been shown to...\n",
            "  - 相似度 0.070: Using murine models, we have shown that the lysosomotropic amine, chloroquine, i...\n",
            "\n",
            "🤖 正在生成答案...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== 9. 準確率評估函數（此格無需修改） ====================\n",
        "\n",
        "def calculate_accuracy(predictions: list, ground_truths: list) -> float:\n",
        "    \"\"\"計算 RAG 系統的準確率\"\"\"\n",
        "\n",
        "    correct = 0\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"開始評估 {NUM_TEST_SAMPLES} 個問題...\")\n",
        "    print('='*60)\n",
        "\n",
        "    for i, ground_truth in enumerate(ground_truths):\n",
        "        try:\n",
        "            answer = predictions[i]\n",
        "            answer_lower = answer.lower()\n",
        "\n",
        "            if ground_truth in answer_lower:\n",
        "                correct += 1\n",
        "                print(f\"✅ 問題 {i+1}: 正確\")\n",
        "            else:\n",
        "                print(f\"❌ 問題 {i+1}: 錯誤 (標準答案: {ground_truth})\")\n",
        "                print(f\"   系統預測: {answer}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ 問題 {i+1}: 發生錯誤 - {e}\")\n",
        "\n",
        "    accuracy = (correct / NUM_TEST_SAMPLES) * 100\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"準確率: {correct}/{NUM_TEST_SAMPLES} = {accuracy:.1f}%\")\n",
        "    print('='*60)\n",
        "\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "pmfilmet2Iel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== 10. 執行模型評估 ====================\n",
        "# 此格理論上無需修改，但前提是模型預測結果之變數名稱需為 `predictions`\n",
        "\n",
        "ground_truths = [item['answer'].lower() for item in test_data[:NUM_TEST_SAMPLES]]\n",
        "accuracy = calculate_accuracy(predictions, ground_truths)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHsUlPAY2KF9",
        "outputId": "c35402a5-af93-4a27-d1fc-dfd14b127557"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "開始評估 10 個問題...\n",
            "============================================================\n",
            "❌ 問題 1: 錯誤 (標準答案: yes)\n",
            "   系統預測: maybe  \n",
            "The excerpt about lace plant leaf perforation describes where PCD occurs but does not mention mitochondria. While mitochondria are commonly involved in programmed cell death signaling in plants, the provided literature does not give direct evidence for their role in lace‑plant leaf remodeling. Hence, based on the available documents the answer is uncertain.\n",
            "✅ 問題 2: 正確\n",
            "❌ 問題 3: 錯誤 (標準答案: yes)\n",
            "   系統預測: maybe\n",
            "The excerpt from Document 1 describes the spatial pattern of programmed cell death in lace plant leaves but does not mention mitochondria or any mitochondrial involvement in the leaf‑remodelling process. Since the provided literature gives no evidence either way, we cannot confirm a role for mitochondria.\n",
            "✅ 問題 4: 正確\n",
            "❌ 問題 5: 錯誤 (標準答案: yes)\n",
            "   系統預測: maybe  \n",
            "The provided excerpt about lace plant leaf perforation describes where PCD occurs but does not mention mitochondria or their involvement. While mitochondria are commonly implicated in programmed cell death in many organisms, the specific role of mitochondria in the leaf remodeling of the lace plant is not addressed in the cited literature, so the answer cannot be confirmed definitively.\n",
            "❌ 問題 6: 錯誤 (標準答案: yes)\n",
            "   系統預測: maybe\n",
            "The excerpt from Document 1 describes the spatial pattern of PCD in lace plant leaf areoles but does not mention mitochondria or any mitochondrial involvement in the leaf‑remodelling process. Since the literature provided gives no evidence either way, we cannot confirm a role, though mitochondria are often implicated in PCD generally. Hence the answer is uncertain.\n",
            "✅ 問題 7: 正確\n",
            "✅ 問題 8: 正確\n",
            "✅ 問題 9: 正確\n",
            "❌ 問題 10: 錯誤 (標準答案: yes)\n",
            "   系統預測: maybe  \n",
            "The excerpt about lace plant leaf perforation describes where PCD occurs but does not mention mitochondria or their involvement in the remodeling process. Since the literature provided gives no evidence either way, we cannot confirm a role, though mitochondria are commonly implicated in plant PCD generally.\n",
            "\n",
            "============================================================\n",
            "準確率: 5/10 = 50.0%\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KIkISazu3sF4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}